<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<title>RTNeuron: Basic usage</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">RTNeuron
   &#160;<span id="projectnumber">3.0.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li class="current"><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
    </ul>
  </div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('basics.html','');});
</script>
<div id="doc-content">
<div class="header">
  <div class="headertitle">
<div class="title">Basic usage </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p> 

<style type="text/css" media="screen, projection">
pre {
    padding: 0.5em;
    border: 1px dashed #2f6fab;
    color: black;
    background-color: #f9f9f9;
    line-height: 1em;
}

#note {
    position: relative;
}
#note span {
    display: none;
}
#note a span {
    display: none;
    color: #FFFFFF;
}
#note a span span {
    display: none;
}
#note a:hover span span {
    display: none;
}
#note a:hover span {
    display: block;
    position: absolute;
    width: 20em;
    background-color: #aaa;
    left: -21em;
    top: -5px;
    color: #FFFFFF;
    padding: 5px;
}
#note a:hover #right {
    display: block;
    position: absolute;
    width: 20em;
    background-color: #aaa;
    left: 2em;
    top: -5px;
    color: #FFFFFF;
    padding: 5px;
}
</style>
</p>
<h1><a class="anchor" id="starting_rtneuron"></a>
Starting RTNeuron</h1>
<p>RTNeuron can be used as a command line application with an embedded Python interpreter or by importing the Python module <code>rtneuron</code> in your own scripts.</p>
<p>In the first case the executable is called <code>rtneuron</code>, which is a Python script itself. When this script is run without arguments it will start the interpreter (IPython if available) with a preloaded environment that includes all RTNeuron classes and several helper functions.</p>
<pre class="fragment">$ rtneuron
No blue config file or model list specified. Launching shell
RTNeuron interactive Python shell
In [1]:
</pre><p>When <code>rtneuron</code> is run with arguments that specify a circuit and several targets no shell is started by default. To start it, include <code>--shell</code> in the command line. For example:</p>
<pre class="fragment">$ rtneuron -b ~/Buildyard/Release/install/include/BBP/BlueConfig --target MiniColumn_0 --shell
Processing 100 neurons to be added
0%   10   20   30   40   50   60   70   80   90   100%
|----|----|----|----|----|----|----|----|----|----|
***************************************************
Adding 100 to the scene
0%   10   20   30   40   50   60   70   80   90   100%
|----|----|----|----|----|----|----|----|----|----|
***************************************************
RTNeuron interactive Python shell
In [1]:
</pre><p>The following sections will cover the basic command line options and Python API to load circuits, display simulation, camera control and record movies.</p>
<h3>Interactive help</h3>
<p>All the reference documentation is available inside the Python console using the <code>help</code> function. You can inspect the reference of RTNeuron classes as well as free functions and methods, e.g: </p><pre class="fragment">$ help(RTNeuron)
$ help(Scene.addNeurons)
$ help(display_circuit)
</pre><h1><a class="anchor" id="basic_concepts"></a>
Basic Concepts</h1>
<p>There are a few basic concepts that are used inside this guide which refer to the main objects handled by an instance of RTNeuron. There is no need to understand what these objects are if you plan to use RTNeuron only from the command line, but if you ever intend to use the Python shell it is worth getting some basic understanding.</p>
<ul>
<li>The <b>engine</b>: This is the main object that holds everything together and handles the Equalizer configuration. The lifetime of views, scenes and the simulation player is subject to the lifetime of the engine.</li>
<li><b>Camera</b>: A camera defines the parameters of the 3D to 2D projection (orthographic or perspective) and the position and orientation of the projection plane (both projections) and projection point (only perspective).</li>
<li><b>Scene</b>: A scene handles the geometric representation of all the objects to be displayed (neurons, synapses and additional polygonal meshes) as well as the simulation data to be mapped on the neurons. This implies that it is not possible to reuse a scene to display two different simulations at the same time. Currently, most shading style properties are scene properties, in particular:<ul>
<li>Whether alpha blending is enabled and the algorithm to use together with its parameters.</li>
<li>Pseudo electron microscopy shading.</li>
</ul>
Objects can be added and removed from a scene, but please note that these operations can be expensive <dl class="section warning"><dt>Warning</dt><dd>In parallel rendering configurations the changes in scenes are currently not propagated from the master node (the node running the interpreter) to the clients.</dd></dl>
Some of the scene responsibilities will be moved to views in the future.</li>
<li><b>Scene</b> <b>object handler</b>: For each object added to the scene, a handler to it is created. An object handler provides the functionality to modify properties like the display mode and color of the object. Changes to scene handlers are applied by modifying an <a href="python/api.html#rtneuron._rtneuron.AttributeMap"><b>rtneuron.AttributeMap</b></a> they contain and then calling an update method.</li>
<li>The <b>simulation player</b>: This objects exposes the methods to change the simulation frame being displayed, start and stop simulation playback and control playback speed.</li>
<li><b>View</b>: A view connects a scene and a camera together. Views handle snapshot capturing and hold some properties that affect the rendering and visual representation of the scene, the most important being:<ul>
<li>The simulation color maps (for compartment and spike data).</li>
<li>Parameters for stereoscopic rendering.</li>
<li>Idle mode anti-aliasing.</li>
<li>The level of detail bias.</li>
</ul>
There is a one to one mapping between RTNeuron views and Equalizer views. That means that if the engine is started with an Equalizer configuration file, a View will be created for each view present in the configuration file.</li>
</ul>
<h2><a class="anchor" id="predefined_objects"></a>
Basic objects in the Python shell</h2>
<p>The objects described above map to objects actually exposed by the Python/C++ library. When RTNeuron is started with something to display and the <code>--shell</code> option, the interpreter contains these predefined variables:</p><ul>
<li><b>engine</b> (<a href="python/api.html#rtneuron._rtneuron.RTNeuron"><b>rtneuron.RTNeuron</b></a>): The RTNeuron engine object. The simulation player API (<a href="python/api.html#rtneuron._rtneuron.SimulationPlayer"><b>rtneuron.SimulationPlayer</b></a>) is accessible through an attribute called <b>player</b>.</li>
<li><b>simulation</b> (brain.Simulation): This is the brain.Simulation representing the simulation config.</li>
<li><b>scene</b> (<a href="python/api.html#rtneuron._rtneuron.Scene"><b>rtneuron.Scene</b></a>): The scene with the targets and models being displayed. The list <b>scene.objects</b> is a read only attribute that contains the handlers for the objects in the scene (<a href="python/api.html#rtneuron._rtneuron.Scene.Object"><b>rtneuron.Scene.Object</b></a>). Each handler has at least two attributes and two methods:<ul>
<li><b>attributes</b>: An <a href="python/api.html#rtneuron._rtneuron.AttributeMap"><b>rtneuron.AttributeMap</b></a> to modify object properties</li>
<li><b>object</b>: A copy of the object used to create the scene object (its type depends on the type of scene object). Modifying it will not affect the scene.</li>
<li><b>update()</b>: The method to make effective the modifications on attributes</li>
<li><b>query(ids)</b>: This method allows the selection of a subset of the entities contained in a handler by means of a list of identifiers, and returns a handler to the subset (only implemented for neuron object handlers at the moment).</li>
</ul>
</li>
<li><b>view</b> (<a href="python/api.html#rtneuron._rtneuron.View"><b>rtneuron.View</b></a>): The first view of the Equalizer configuration loaded (and the only one if the default one is used). The camera is accessed by <b>view.camera</b> (<a href="python/api.html#rtneuron._rtneuron.Camera"><b>rtneuron.Camera</b></a>).</li>
</ul>
<p>More classes are available and will be introduced in the following sections. You can always refer to the <a href="annotated.html">C++</a> or <a href="python/python_doc.html">Python</a> library reference manuals for a more technical description.</p>
<h1><a class="anchor" id="command_line_scenes"></a>
Displaying objects using the command line</h1>
<p>Currently, the only objects that can be displayed with <code>rtneuron</code> from the command line are neuron targets. To do so, the first thing that needs to be given is the BlueConfig file for that circuit. This is done by passing the path to the file with the <code>-b</code> command line option at start-up.</p>
<p>To specify the neuron targets to display there are three command line options</p><ul>
<li><code>--target</code> <em>target_name</em> [<em>options</em>]</li>
<li><code>--neurons</code> <em>first_gid</em> <em>last_gid</em> [<em>options</em>]</li>
<li><code>-n</code> <em>gid</em> [<em>options</em>]</li>
</ul>
<p>The first option loads a neuron set by its target name as present in the <code>user.target</code> or <code>start.target</code> files pointed by the BlueConfig. Regular expressiona are also accepted, as well as the special suffix <code>%number</code> to request a subsampling of the target to the given percentage (e.g. Column%5). The second option loads a list of neurons by GID starting at the first GID and finishing at the second one, both included. The last option allows to load a single neuron given its GID. The optional parameters for each option are explained <a class="el" href="basics.html#neuron_target_options">below</a>.</p>
<p>These options can be combined in any order any number of times. Note however, that when a target is added to the scene RTNeuron will refuse to add the target and will print an error if some of its neurons have already been added to the scene.</p>
<p>After loading, the camera will be automatically placed along the z axis in a position where all the cell somas are encompassed in vertical  <span id="note"><a><sup>note</sup><span id="right"><span>(</span>
This is true as long as no other parameters affecting the camera are included.
<span>)</span></span></a></span>.</p>
<p>If no additional options to create a specific Equalizer configuration are provided, RTNeuron will start a default configuration when started this way. The image below shows the default window with a soma column of 10K neurons loaded from the command line:</p>
<div class="image">
<img src="startup_with_a_soma_column.png" alt="startup_with_a_soma_column.png"/>
<div class="caption">
Default RTNeuron window launched with: rtneuron -b BlueConfig &ndash;target Column soma.</div></div>
<h1><a class="anchor" id="interpreter_scenes"></a>
Displaying objects using the Python shell</h1>
<p>Inside the python shell the objects that can be added to scenes are:</p><ul>
<li>Neuron targets</li>
<li>Synapse targets</li>
<li>Polygonal meshes, which can be either:<ul>
<li>Polygonal models loaded from file.</li>
<li>Arbitrary meshes created programmatically.</li>
</ul>
</li>
</ul>
<p>In general, objects can be added to the scene using helper functions provided by the <code>rtneuron</code> module or using the Python API manually.</p>
<dl class="section warning"><dt>Warning</dt><dd>Limitations apply to multi-node Equalizer configurations. At the moment scene creation and propagation of changes are not redistributed to clients. Only a few things are redistributed and that is the reason for which scenes must be created before the configuration is started. This may change in the future, but the reason for this restriction is that scenes must be known before a distributed configuration is started so it is possible to redistribute the changes that are supported at the moment.</dd></dl>
<h2><a class="anchor" id="helper_functions_scenes"></a>
Helper Python functions to populate scenes</h2>
<h3>Displaying neuron targets</h3>
<p>The function that eases the loading and displaying of neuron targets is <a href="python/api.html#rtneuron.display_circuit"><b>rtneuron.display_circuit</b></a>. It can used inside the Python shell either started with <code>rtneuron</code> without arguments or using the <code>--shell</code> option.</p>
<ul>
<li><a href="python/api.html#rtneuron.display_circuit"><b>rtneuron.display_circuit(<em>blue_config, target=('Column', {'mode': rtneuron.RepresentationMode.SOMA}), report=None, spikes=None, eq_config='', argv=None</em>)</b></a></li>
</ul>
<p>Only the first 2 arguments will be explained here, the rest will be explained in the following sections. The first argument is a path to a simulation config file that will be opened by a brain.Simulation. The second argument is a specification of neuron targets to load and display. This argument can be a <em>target</em> or a list of targets. A <em>target</em> can be either a <em>target key</em> or a tuple (<em>target key</em>, <em>target attributes</em>). <em>Target keys</em> can be of one of these types:</p><ul>
<li><code>int:</code> a cell GID</li>
<li><code>numpy</code> arrays of dtype u4, u8 or i4</li>
<li><code>str:</code> A target name or regular expression for targets from the <code>start.target</code> or <code>user.target</code> files with an optional suffix in the format <code>%number</code> to reduce the target to the given percentage.</li>
</ul>
<p>The <em>target attributes</em> can be a Python dict of (name, value) pairs or an <a href="python/api.html#rtneuron._rtneuron.AttributeMap"><b>rtneuron.AttributeMap</b></a>. Possible attributes are documented in <a href="python/api.html#rtneuron._rtneuron.Scene.addNeurons"><b>rtneuron.Scene.addNeurons</b></a> and in the next section.</p>
<p>This function affects global variables of the <code>rtneuron</code> module (the variables from the interpreter that reference the same objects are also updated):</p><ul>
<li><b>engine</b>: the <a href="python/api.html#rtneuron._rtneuron.RTNeuron"><b>rtneuron.RTNeuron</b></a> object. If already existing, the current configuration is exited before anything else. If needed a new engine is created.</li>
<li><b>simulation</b>: the brain.Simulation, replaced with a new one using the blue config file given.</li>
<li><b>scene</b>: Replaced with the new one.</li>
<li><b>view</b>: Replaced with first view of the new configuration.</li>
</ul>
<h3>Displaying synapse targets</h3>
<p>Synapses are displayed as spherical glyphs located at the presynaptic (<em>efferent</em>) or postsynaptic (<em>afferent</em>) site of the synapse.</p>
<p>There are two functions to add synapses to the scene:</p><ul>
<li><a href="python/api.html#rtneuron.display_synapses"><b>rtneuron.display_synapses(<em>target, afferent=True, attributes=None</em>)</b></a></li>
<li><a href="python/api.html#rtneuron.display_shared_synapses"><b>rtneuron.display_shared_synapses(<em>presynaptic, postsynaptic, afferent=True, attributes=None</em>)</b></a></li>
</ul>
<p>Both functions assume that an RTNeuron instance with a valid configuration and scene is already running and will add the synapses to the <b>scene</b> attached to the first <b>view</b>. These preconditions are guaranteed if a circuit is loaded from the command line or using <a href="python/api.html#rtneuron.display_circuit"><b>rtneuron.display_circuit</b></a>.</p>
<dl class="section note"><dt>Note</dt><dd>If you only want to display synapses, the easiest way to accomplish this is either remove the neurons from the scene with <a href="python/api.html#rtneuron._rtneuron.Scene.remove"><b>rtneuron.Scene.remove</b></a> or hide them changing their display mode to RepresentationMode.NO_DISPLAY. Details on how to do the latter are given in <a class="el" href="basics.html#object_attributes">Scene object attributes</a>. This process will be improved and simplified in the future.</dd>
<dd>
Load balancing mechanisms for sort-last parallel configurations are not implemented for synapse glyphs.</dd></dl>
<p>The neurons and morphologies required to compute the synapse locations must have also been loaded. If a neuron is not available, an exception will be thrown. If a required morphology is not available, the synapse for which the location cannot be computed will be skipped and a warning message will be printed.</p>
<p>These functions load the required synapses automatically into the microcircuit. </p><dl class="section warning"><dt>Warning</dt><dd>All bbp.Synapses containers already existing before calling either function will be invalidated (despite its inconvenience, this is by design of the BBPSDK).</dd></dl>
<p>The <em>afferent</em> parameter can be used to choose between showing the postsynaptic (true) or the presynaptic positions (false) of the synapses. The attribute map can be used to change the positioning and the color and radius initially assigned to the synapses (more details <a class="el" href="basics.html#synapse_attributes">here</a>).</p>
<table  class="collapsed_table">
<tr>
<td><div class="image">
<img src="afferent_synapses.png" alt="afferent_synapses.png"/>
<div class="caption">
Afferent synapses of a single neuron. Added with display_synapses(neuron).</div></div>
 </td><td><div class="image">
<img src="shared_synapses.png" alt="shared_synapses.png"/>
<div class="caption">
Pre and postsynaptic positions of 3 synapses shared between 2 neurons. Added with display_shared_synapses(pre, post, afferent = True|False, attributes = AttributeMap({'color': color)))</div></div>
   </td></tr>
</table>
<h3>Displaying additional models</h3>
<p>Adding models from files to a scene is straight forward, for that reason there are not helper functions to do it.</p>
<p>For arbitrary meshes created by the user there is no general helper function, but there exists a function to add hexagonal prisms to the scene.</p>
<ul>
<li><p class="startli"><a href="python/api.html#rtneuron.add_hexagonal_prism"><b>rtneuron.add_hexagonal_prism(<em>scene, center, height, radius, color=[0.2, 0.4, 1.0, 0.2], line_width=2.5</em>)</b></a></p>
<p class="startli">This function adds two objects to the <em>scene</em> passed as first argument. The first one is a hexagonal prism where the center of the bottom hexagon is placed at <em>center</em>, the length of the hexagon sides is <em>radius</em>, the height is <em>height</em> and the color is <em>color</em>. The second model is the same prism but rendered as a black wireframe on top of the first one using <em>line_width</em> as line width. The 3D vector for the center must be an indexable object of length 3.</p>
</li>
</ul>
<p>The image below shows 6 of these prisms rendered with alpha-blending enabled (<code>--alpha-blending</code> command line option).</p>
<div class="image">
<img src="hexagonal_prisms.png" alt="hexagonal_prisms.png"/>
</div>
<h2><a class="anchor" id="manual_scenes"></a>
Creating, populating and modifying scenes manually</h2>
<p>Scenes can only be created using the method <a href="python/api.html#rtneuron._rtneuron.RTNeuron.createScene"><b>rtneuron.RTNeuron.createScene</b></a> and before the Equalizer configuration is started by the engine object. The method createScene can take an optional <a class="el" href="classbbp_1_1rtneuron_1_1AttributeMap.html" title="An attribute map that stores arbitrary values using strings as keys. ">AttributeMap</a> to override the default scene parameters. Some of these options are explained in the <a class="el" href="advanced.html">Advanced usage and recipes</a> as well at the reference manual.</p>
<p>Once a scene is created objects can be added or removed at any moment. Note however, that it is better to modify a scene when it is not attached to any view because scene modifications trigger potentially costly operations. Automatic scenes updates can be enabled and disable with the scene attribute called <code><b>auto_update</b></code> and triggered manually with <a href="python/api.html#rtneuron._rtneuron.Scene::update"><b>rtneuron.Scene::update</b></a>. However, note that scenes are also updated when a frame is triggered by any other reason (e.g. camera manipulation with the mouse).</p>
<p>The functions used to add/remove objects to a scene can be applied to scenes that have been created either from the command line or using <code><b>display_circuit</b></code>. Each object type is added using a different function, but all can be removed passing the proper object handler to the <a class="el" href="classbbp_1_1rtneuron_1_1Scene.html#a329d0a163983de5f738a314ce5f25367" title="Removes a target/model from the scene given its handler. ">Scene.remove</a>. Functions to add objects to the scene always return a handler to the object added. All the objects contained inside a scene can be accessed using the property <a href="python/api.html#rtneuron._rtneuron.Scene.objects"><b>rtneuron.Scene.objects</b></a>.</p>
<h3>Adding neuron targets</h3>
<p>To add a neuron target the method to use is <a href="python/api.html#rtneuron._rtneuron.Scene.addNeurons"><b>rtneuron.Scene.addNeurons</b></a> which takes a bbp.Neurons neurons container and an optional attribute map to configure the representation (see the reference for details) and returns an <a href="python/api.html#rtneuron._rtneuron.Scene.Object">object handler</a> which can be used to modify the target afterwards.</p>
<p>The <b>object</b> property of the handler returns the bbp.Neurons container with the neurons added to the scene.</p>
<h3>Adding synapse targets</h3>
<p>To add synapse glyphs to the scene the methods to use are <a href="python/api.html#rtneuron._rtneuron.Scene.addAfferentSynapses"><b>rtneuron.Scene.addAfferentSynapses</b></a> and <a href="python/api.html#rtneuron._rtneuron.Scene.addEfferentSynapses"><b>rtneuron.Scene.addEfferentSynapses</b></a> Each one takes a bbp.Synapses container and adds spherical glyphs at the locations of the synapses, returning the object handler to modify and remove the synapses. The first function uses the presynaptic locations reported by the circuit file and the second one uses the postsynaptic locations. The afferent locations of soma synapses is computed projecting the efferent position on the approximate soma sphere orthogonally.</p>
<p>The <b>object</b> property of the handler returns the bbp.Synapses container with the synapses added to the scene.</p>
<dl class="section warning"><dt>Warning</dt><dd>As already mentioned, note that this container may have been invalidated by having called the load function from Microcircuit. BBPSDK does not provide any mechanism to protect against faulty memory accesses if an invalid container is used.</dd></dl>
<h3>Adding other polygonal models</h3>
<p>There are two functions to add polygonal models to the scene, one that loads the model from a file and another in which the model is provided by the user as a mesh with optional colors and normals.</p>
<p>To load models from files use <a href="python/api.html#rtneuron._rtneuron.Scene.addModel"><b>rtneuron.Scene.addModel</b></a>. There are two overloads for this function and both take a file name, a transformation and an attribute map. The difference lies in the transformation parameter. In one overload it is a bbp.Matrix4f and in the other it is a string that specifies a colon separated concatenation of transformations. The transformations are written as:</p><ul>
<li>"r@x,y,z,angle" for rotations, angle in degrees.</li>
<li>"s@x,y,z" for scalings.</li>
<li>"t@x,y,z" for translations.</li>
</ul>
<p>An example is "r@1,0,0,90:t@1000,0,0". The supported file formats are those loadable by OpenSceneGraph plugins.</p>
<p>Arbitrary meshes are added with <a href="python/api.html#rtneuron._rtneuron.Scene.addMesh"><b>rtneuron.Scene.addMesh</b></a>. This function can add to the scene triangle and line soups. The first parameter is a n&times;3 array with the vertices, the input type can be an iterable of iterables or a numpy array. The second is the primitive to draw, it can be either a list of triangles or line indices. In the first case the functions expects a n&times;3 array or similar, and a n&times;2 array in the second. The forth parameter is optional and it is a n&times;3 array with the per vertex normals. The fifth is a n&times;4 array with per vertex colors. The last parameter, also optional, is an attribute map with options for shading and choosing the primitive type. The valid attributes are explained in <a href="python/api.html#rtneuron._rtneuron.Scene.addMesh"><b>rtneuron.Scene.addMesh</b></a>.</p>
<h1><a class="anchor" id="object_attributes"></a>
Scene object attributes</h1>
<h2><a class="anchor" id="neuron_target_options"></a>
Neuron targets attributes</h2>
<p>There are two properties of a neuron target that can be configured from the command line or the Python API functions to add neurons to the scene: the display mode and the coloring mode. Using Python API there is access to more properties and actions. For example, it is also possible to cull away sections based on their branch order and modify the color maps using for simulation mapping (<a class="el" href="basics.html#per_object_colormaps">Per object color maps</a>).</p>
<p> 
<table id="neuron_target_prop">
<tr>
<th colspan="4">Display modes: How neurons morphologies are displayed</th>
</tr>

<tr>
<th>Name</th><th>Description</th>
<th>Python value</th><th>CLI
<span id="note"><a><sup>note</sup><span><span>(</span>
Command line interface name<span>)</span></span></a></span></th>
</tr>

<tr>
<td class="name">Soma</td>
<td class="description">
  Only the soma of the neurons will be shown using a sphere.
  Neuron morphologies will be checked to obtain the radius of each neuron
  unless <code>--no-morphologies</code> is used.</td>
<td class="python_value">RepresentationMode.SOMA</td>
<td class="cli_token">soma</td>
</tr>

<tr>
<td class="name" style="padding-top: 15px">Detailed mode</td>
<td class="description">
  Load and display the neurons normally
  <span id="note"><a><sup>note</sup><span><span>(</span>
  What normally means depens on other options, but in general this means using
  all levels of detail available, including the highly detailed meshes.
  <span>)</span></span></a></span>. Apart from the circuit data and
  the morphologies, meshes will also be loaded for the neuron target in this
  case.</td>
<td class="python_value" style="padding-top: 15px">
RepresentationMode.WHOLE_NEURON</td>
<td class="cli_token" style="padding-top: 15px">detailed</td>
</tr>

<tr>
<td class="name" style="padding-top: 15px">Detailed without axon</td>
<td class="description">
  A variation of the detailed mode in which the axon is removed from the model
  <span id="note"><a><sup>note</sup><span><span>(</span>
  Unless unique morphologies can be assumed the axon is not actually removed,
  but culled during rendering.
  <span>)</span></span></a></span>.
 </td>
<td class="python_value" style="padding-top: 15px">
RepresentationMode.NO_AXON</td>
<td class="cli_token" style="padding-top: 15px">no_axon</td>
</tr>

<tr>
<td class="name">Skeleton</td>
<td class="description">
  Display neurons using a graphical representation based solely on the
  morphological points and radii.
  <dl class="section note"><dt>Note</dt><dd>
  This mode is intended for mesh generation debugging. It is not recommended
  to use it for more that a handful of neurons because of its memory
  consumption and performance. If you need to display a circuit for which
  meshes do not exist use the command line option --no-meshes instead.
  </dd></dl>
</td>
<td class="python_value">RepresentationMode.SEGMENT_SKELETON</td>
<td class="cli_token">skeleton</td>
</tr>

<tr>
<td class="name">No display</td>
<td class="description">
  The neurons will not be displayed but their information will still be
  loaded.</td>
<td class="python_value">RepresentationMode.NO_DISPLAY</td>
<td class="cli_token">none</td>
</tr>
</table>
<p />
</p>
<table  id="neuron_target_prop_illustration">
<tr>
<td><div class="image">
<img src="cell_soma.png" alt="cell_soma.png"/>
</div>
 </td><td><div class="image">
<img src="cell_detailed.png" alt="cell_detailed.png"/>
</div>
 </td><td><div class="image">
<img src="cell_no_axon.png" alt="cell_no_axon.png"/>
</div>
 </td><td><div class="image">
<img src="cell_skeleton.png" alt="cell_skeleton.png"/>
</div>
 </td></tr>
<tr>
<td>Soma</td><td>Detailed</td><td>No axon</td><td>Skeleton only  </td></tr>
</table>
<p> 
<p />
<table id="neuron_target_prop">
<tr>
<th colspan="4">Coloring schemes: How neurons are colored</th>
</tr>

<tr>
<th>Name</th><th>Description</th>
<th>Python value</th><th style="width: 24em;">CLI
<span id="note"><a><sup>note</sup><span><span>(</span>
Command line interface name<span>)</span></span></a></span></th>
</tr>

<tr>
<td class="name">Solid</td>
<td class="description">Use a single user given color.</td>
<td class="python_value">ColorScheme.SOLID</td>
<td class="cli_token">R G B [A]</td>
</tr>

<tr>
<td class="name">Random</td>
<td class="description">Use a single random opaque color.</td>
<td class="python_value">-</td>
<td class="cli_token">random</td>
</tr>

<tr>
<td class="name">All random</td>
<td class="description">Use a random color opaque for each neuron.</td>
<td class="python_value">ColorScheme.RANDOM</td>
<td class="cli_token">all-random</td>
</tr>

<tr>
<td class="name">Color by branch type</td>
<td class="description">Color the dendrites and soma with one color and
the axon with other. Unless two full RBGA tuples are provided, the default
color for dendrites and soma is red and the axon color is blue.</td>
<td class="python_value">ColorScheme.BY_BRANCH</td>
<td class="cli_token">by-branch [R G B A  R G B A]</td>
</tr>

<tr>
<td class="name" style="padding-top: 15px;">Width dependent alpha channel</td>
<td class="description">
   <p>Blend between two colors based on the width of the neuron branches. The
   blend is a non-linear interpolation that tries to imitate translucency
   <span id="note"><a><sup>note</sup><span><span>(</span>
   In fact the blend is a linear interpolation between the two colors using
   a = 1 - exp(-width * 1/attenuation) as the interpolation coefficient. The
   default attenuation is 2<span>)</span></span></a></span>. The blended
   colors are computed from a primary and a secondary color. For zero width
   branches the color to use is the primary color. For infinite width branches
   the color is the secondary color. In practice, the non-linear blend
   guarantees that the secondary color is the color used for thick dendrites
   and the soma.</p>
   <p>
   The optional attenuation factor determines how quickly the color
   interpolation falls from the seconday to the primary color as a function of
   the width. It takes a real value >= 0 and if not provided defaults to 2.
   </p>
</td>
<td class="python_value">ColorScheme.BY_WIDTH</td>
<td class="cli_token">by-width[@attenuation] [R G B [A]]</td>
</tr>
</table>
<p />
</p>
<table  id="neuron_target_prop_illustration">
<tr>
<td><div class="image">
<img src="minicolumn_solid.png" alt="minicolumn_solid.png"/>
</div>
 </td><td><div class="image">
<img src="minicolumn_color_by_branch.png" alt="minicolumn_color_by_branch.png"/>
</div>
 </td><td><div class="image">
<img src="minicolumn_alpha_by_width.png" alt="minicolumn_alpha_by_width.png"/>
</div>
  </td></tr>
<tr>
<td>Solid</td><td>Color by branch type</td><td>Width dependent alpha channel </td></tr>
</table>
<h3>Neuron target attributes in the command line</h3>
<p>In the command line, neuron target properties are given to the <code>-n</code>, <code>--neurons</code> and <code>--target</code> after. The full specification is: </p><table  style="border: 0px; margin-left: auto; margin-right: auto">
<tr>
<td style="text-align: right;"><em>options</em> </td><td>=</td><td>[<em>display_mode</em> [<em>coloring</em>] ] </td></tr>
<tr>
<td style="text-align: right;"><em>display_mode</em> </td><td>=</td><td><code>soma</code> | <code>detailed</code> | <code>no_axon</code> | <code>skeleton</code> | <code>none</code>  </td></tr>
<tr>
<td style="text-align: right;"><em>coloring</em> </td><td>=</td><td><em>RGB</em>[<em>A</em>] | <code>by-branch</code> | <code>alpha-by-width</code>[@attenuation] [<em>RGB</em>[<em>A</em>]] </td></tr>
</table>
<p>Regarding the coloring options, the solid coloring scheme is used if just an <em>RGB</em>[<em>A</em>] tuple is given. For <code>alpha-by-width</code>, if no optional primary color is given, (0, 0.5, 1.0, 0.0) will be used by default. The secondary color cannot be changed from the command line, instead it is computed as the primary color plus (0.5, 0.5, 0.5) added to the RGB channels, clamping each channel to 1.0; the A channel is set to 1. If no attenuation value is given, it defaults to 2.</p>
<h3>Neuron target attributes in the Python shell</h3>
<p>From a scene object that handles a neuron target it is possible to change:</p><ul>
<li>The representation mode.</li>
<li>The coloring scheme and base color.</li>
<li>the color maps used for simulation mapping. These attribute are accessible from <a href="python/api.html#rtneuron._rtneuron.Scene.Object.attributes"><b>rtneuron.Scene.Object.attributes</b></a>. The display mode is <code><b>mode</b></code>, the color scheme is <code><b>color_scheme</b></code>, the base color for the solid and by-width schemes is <code><b>color</b></code> or <code><b>primary_color</b></code> and the secondary color for by-width is <code><b>secondary_color</b></code>. The attribute values are those listed in the two tables above. Base colors are specified as lists with 3 or 4 float numbers in the range [0..1]. To change which is the maximum branch order of the sections that will be visible assign integer values to <code><b>max_visible_branch_order</b></code>, the special value -1 is reserved to mean that all sections are visible.</li>
</ul>
<p>For color schemes that accept additional parameters, these parameters can be provided assigning an additional <a class="el" href="classbbp_1_1rtneuron_1_1AttributeMap.html" title="An attribute map that stores arbitrary values using strings as keys. ">AttributeMap</a> to the attribute <code><b>extra</b></code>. In particular, <code>alpha-by-width</code> accepts the attribute <code><b>attenuation</b></code> to modify the equation that modulates the color interpolation.</p>
<p>Color maps are collected in an attribute called <code><b>colormaps</b></code>. This attribute is unset by default, implying that the colormaps are taken from the <a class="el" href="classbbp_1_1rtneuron_1_1View.html" title="This class represents a view on a scene. ">View</a> object. This attribute can receive an AtributeMap in which different <a href="python/api.html#rtneuron._rtneuron.ColorMap"><b>rtneuron.ColorMap</b></a>s are assigned. In particular you can set color maps on the attributes <code><b>compartments</b></code> and <code><b>spikes</b></code>. When the attribute map is reset, the color maps are cleared. For more details about color maps see <a class="el" href="basics.html#simulation_colormaps">Color maps</a> and <a class="el" href="basics.html#per_object_colormaps">Per object color maps</a>.</p>
<p>After changes to the attribute map are made, they become effective after calling the <a href="python/api.html#rtneuron._rtneuron.Scene.Object.update"><b>update</b></a> method of the handler.</p>
<p>For example, given a neuron target handler <b>n</b> the code to change the display mode to soma and paint it red the code would be: </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;$ n.attributes.color = [1, 0, 0]</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;$ n.attributes.color_scheme = ColorScheme.SOLID</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;$ n.attributes.mode = RepresentionMode.SOMA</div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;$ n.update()</div></div><!-- fragment --><p>For changing the color scheme to be by-width with an attenuation factor of 1.2 </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;$ n.attributes.color_scheme = ColorScheme.BY_WIDTH</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;$ n.attributes.extra = AttributeMap()</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;$ n.attributes.extra.attenuation = 1.2</div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;$ n.update()</div></div><!-- fragment --><dl class="section warning"><dt>Warning</dt><dd>These modifications have some caveats:<ul>
<li>A neuron target that was added to the scene using <b>no_axon</b> cannot be changed to <b>detailed</b> afterwards.</li>
<li>A neuron target that was added to the scene using <b>soma</b> can be upgraded to <b>detailed</b> or <b>no_axon</b>, but that will trigger the creation of the additional models.</li>
</ul>
</dd></dl>
<h2><a class="anchor" id="synapse_attributes"></a>
Synapse target attributes</h2>
<p>Synapse targets attributes can only be specified in the Python shell. This attributes can be passed to the functions that add synapses to the scene and can be modified later on using the scene object handler (modifying the values of <a href="python/api.html#rtneuron._rtneuron.Scene.Object.attributes"><b>rtneuron.Scene.Object.attributes</b></a>). The available attributes are the <code><b>radius</b></code>, <code><b>color</b></code> and <code><b>surface</b></code>. The radius units are microns and the default value is 1. The default color is (1, 0.6, 0.2, 1) for afferent and (0.5, 0.2, 1.0, 1) for efferent synapses. The surface attribute toggles the positioning of the synapses on either the surface of the geometry or in the center. Note that the position of the synapses cannot be changed after they were added to the scene.</p>
<p>After changes to the attribute map are made, they become effective after calling the <a href="python/api.html#rtneuron._rtneuron.Scene.Object.update"><b>update</b></a> method of the handler.</p>
<h1><a class="anchor" id="simulation"></a>
Displaying simulation</h1>
<h2><a class="anchor" id="simulation_types"></a>
Simulation data types</h2>
<p>Two types of simulation reports are supported by RTNeuron: compartment reports from Bluron and spike time reports from Bluron or NEST. At the moment simulation data is a scene property and only one report of each type can be mapped to a scene at a given time.</p>
<p>Compartment reports are the main reports produced by the simulator. These reports contain scalar simulation values for the electrical compartments of the cell dendrites and somas (and a small portion of the axon). The values are mapped onto cells using an indexing scheme provided by the simulation.</p>
<dl class="section note"><dt>Note</dt><dd>By default, report frames are loaded on demand and only one frame is kept in memory at a time. This keeps the memory footprint low, but increases the latency to get the next frame, this effect is quite noticeable with large simulations (&gt;1K neurons) when loading from a regular SATA drive. If enough host memory is available, this problem can be overcome from the python shell, calling the method <code>preload</code>(<em>begin</em>, <code><em>end</em></code>) from <code>bbp.Compartment_Report_Reader</code>, which will load in memory all the frames within the given time window. As of now, there is no way to access the reader created by <code>rtneuron</code> and attached to the scene, so the solution is to do the whole process manually.</dd></dl>
<p>A typical voltage compartment report mapped onto cell membranes looks like the following picture:</p>
<div class="image">
<img src="voltage_report.png" alt="voltage_report.png"/>
</div>
<p>Spike reports are a collection of pairs of spike times and neuron GIDs. This information is used to show how the action potential propagates along the axon. By default, the visual representation is an engrossment and color change from black to white that travels along the axon followed by a decay back to normal. The color map can be changed as described <a class="el" href="basics.html#simulation_colormaps">below</a>. The action potential travels at a fixed propagation delay of 300 um/ms. The decay tail can be configured by the user. The following figure shows a spike at the beginning of the axon with 2 different spike tail lengths.</p>
<div class="image">
<img src="spikes.png" alt="spikes.png"/>
<div class="caption">
Visualization of spikes using two different decay times.</div></div>
<p>When a spike report has been loaded but no compartment report has been loaded, the spike data will be used to make somas flash when a spike occurs. By default, somas are rendered black (transparent is alpha blending is enabled) at resting state and they become white (and fully opaque) at spike times, gradually turning back into black during the spike tail time.</p>
<h2><a class="anchor" id="load_simulation"></a>
Loading simulation data</h2>
<h3>From the command line</h3>
<p>To specify a compartment report, use the command line option <code>-r</code>, <code>--report</code>. If no report with that name exists in the BlueConfig, or the report is not accessible, it will be ignored.</p>
<p>Spikes are enabled using the command line option <code>-s</code>, <code>--spikes</code>. If this option is provided, RTNeuron will query the content of the <code>SpikesPath</code> field of the blue config loaded and load the spikes from there. A path to a file can also be provided explicitly following <code>-s/<code>--spikes</code>.</code> For Bluron reports, the expected file extension is <code></code>.dat. For NEST reports, the extension is <code></code>.gdf and shell wildcards are also accepted (* and ?). Wildcards must be passed to RTNeuron and not interpreted by the shell, so if the path contains wildcards, the argument must be quoted in the command line invocation.</p>
<h3>In the Python shell</h3>
<p>Using <a href="python/api.html#rtneuron.display_circuit"><b>rtneuron.display_circuit</b></a>, compartment and spike reports are loaded by passing the report name and the spike file name as the <code>report</code> and <code>spikes</code> arguments respectively.</p>
<p>From the console it is possible to change the reports being displayed at any moment. Reports can be assigned to scenes using <a href="python/api.html#rtneuron._rtneuron.Scene.setSimulation"><b>rtneuron.Scene.setSimulation</b></a>, which is overloaded to accept either a <a href="https://developer.humanbrainproject.eu/docs/BBPSDK/latest/classbbp_1_1_compartment_report_reader.html"><b>bbp.CompartmentReportReader</b> </a> or a <a href="https://developer.humanbrainproject.eu/docs/BBPSDK/latest/classbbp_1_1_spike_report_reader.html"><b>bbp.SpikeReportReader</b> </a> object. To facilitate the process, the <code>rtneuron</code> module provides two helper functions:</p>
<ul>
<li><p class="startli"><a href="python/api.html#rtneuron.apply_compartment_report"><b>rtneuron.apply_compartment_report(<em>simulation, scene_or_view, report_name</em>)</b></a></p>
<p class="startli">This function creates a compartment report reader for the given simulation and report name and attaches it to a scene or the scene associated with a view.</p>
</li>
<li><p class="startli"><a href="python/api.html#rtneuron.apply_spike_data"><b>rtneuron.apply_spike_data(<em>simulation_or_filename, scene_or_view</em>)</b></a></p>
<p class="startli">This function creates a spike report reader for a file path or the spike data referenced by an simulation and attaches it to a scene or the view associated with a scene.</p>
</li>
</ul>
<p>The Python shell has some predefined variables that can be used as input parameters for both functions (in your own scripts it is your responsibility to create them), see the description of the <a class="el" href="basics.html#predefined_objects">predefined variables</a> for reference.</p>
<p>Simulation data readers can also be created and attached manually. See the BBPSDK reference for <a href="https://developer.humanbrainproject.eu/docs/BBPSDK/latest/classbbp_1_1_compartment_report_reader.html">compartment</a> and <a href="https://developer.humanbrainproject.eu/docs/BBPSDK/latest/classbbp_1_1_spike_report_reader.html">spike</a> report readers.</p>
<h2><a class="anchor" id="simulation_playback"></a>
Simulation display and playback</h2>
<p>With <code>rtneuron</code>, simulation playback is started automatically after the targets are loaded. Simulation is played back until the end and then stops. The time window of interest can be selected with the command line options <code>--sim-window</code> (<code>-w</code>) and the simulation speed with <code>--sim-step</code>. The actual interest window is computed as the intersection of the user given window and the information retrieved from the BlueConfig for the loaded report. For spike visualization, the user can select during how much time there must be a visual indication of a spike in the soma or along the axon, this time is the <em>spike tail</em>. In somas this indications is gradual color change from white (a spike just occurred) to black (no spike). On the axons, the spike leaves a trail as it travels the axon, the length depends on the spike tail time. The spike tail can be adjusted from the command line with the option. <code>--spike-tail</code>.</p>
<p>Finer control on playback is provided by the Python API. The object <a href="python/api.html#rtneuron._rtneuron.SimulationPlayer"><b>rtneuron.SimulationPlayer</b></a> (<b>app.player</b> in the embedded shell) provides the interface to simulation playback. All parameters are controlled from there except toggling simulation display on and off, spike trails and color maps (see below), which are <a href="python/api.html#rtneuron._rtneuron.View"><b>rtneuron.View</b></a> attributes and properties. Refer to the reference manual or the interactive help for details.</p>
<h2><a class="anchor" id="simulation_colormaps"></a>
Color maps</h2>
<p>Each view has two color maps to translate compartmental simulation values and spikes into colors. A color map is defined by a list of control points which maps discrete values to colors. For values that fall between two control points, the color assigned is a linear interpolation between the colors of the control points just above and below that value. Values above the maximum and below the minimum of the control points take the color of those control points respectively.</p>
<p>For compartment reports, the color map is sampled using the simulation values. For spike reports the color map is expected to be defined in the range 0 to 1. Given the current timestamp <code>t</code> and the time of a spike <code>t_s</code>, the value <code>a</code> = (<code>t</code> - <code>t_s</code>) / <code>spike_tail</code> is computed. That is the value used to sample the color map, clamped to [0, 1] if necessary.</p>
<p>The default color map for compartmental data has been crafted to display membrane voltage and is shown in the image below.</p>
<div class="image">
<img src="default_colormap.png" alt="default_colormap.png"/>
<div class="caption">
Default color map. The color map is defined in the range -80 to -10. The bottom half shows the color map with opaque colors and the upper half with the alpha channel enabled over a checkerboard background.</div></div>
<p>The color map can only be changed using the Python API. For that purpose there is a <a href="python/api.html#rtneuron._rtneuron.ColorMap"><b>rtneuron.ColorMap</b></a> class. This class has a method <a href="python/api.html#rtneuron._rtneuron.ColorMap.setPoints"><b>setPoints</b></a> that takes a dictionary of (value, RGBA) pairs to define the control points (colors must always be 4-element tuples, i.e. the alpha value can't be omitted). Once the control points are established, they cannot be changed (only completely replaced).</p>
<p>Views handle color maps using an attribute called <code><b>colormaps</b></code>. This attribute is an <a class="el" href="classbbp_1_1rtneuron_1_1AttributeMap.html" title="An attribute map that stores arbitrary values using strings as keys. ">AttributeMap</a> itself and it contains two attributes. The first attribute is called <code><b>compartments</b></code> and it contains the <a href="python/api.html#rtneuron._rtneuron.ColorMap"><b>rtneuron.ColorMap</b></a> that is used to assign colors to compartmental simulation data. The second attribute is <code><b>spikes</b></code> and contains the colormap used to render spikes. You can modifiy the color maps directly or assign new ColorMaps to the mentioned attributes. It is an error trying to assign any other value different from a <a href="python/api.html#rtneuron._rtneuron.ColorMap"><b>rtneuron.ColorMap</b></a> to a colormap attribute. The view will detect any changes in the color maps and trigger the rendering automatically to show the new color map. The same color map can be assigned to multiple views. The color map is applied to render a <a href="python/api.html#rtneuron._rtneuron.View"><b>rtneuron.View</b></a> only when the <code><b>display_simulation</b></code> attribute of that view is set to on, otherwise, the target coloring attributes will be used.</p>
<p>The code below shows how to create a color map that goes from transparent black at -65 to opaque green at -40 and enable it in the predefined view: </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;c = ColorMap()</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;c.setPoints({-65: [0, 0, 0, 0], -40: [0, 1, 0, 1]})</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;view.colormaps.compartments = c</div></div><!-- fragment --><p>The image below shows this color map compared to the default ones, with transparency enabled for both: </p><a class="anchor" id=""></a>
<table  class="collapsed_table">
<caption>Comparison of the same simulation frame using two different color maps with transparency.</caption>
<tr>
<td><div class="image">
<img src="colormap_default.png" alt="colormap_default.png"/>
<div class="caption">
Default, voltages</div></div>
 </td><td><div class="image">
<img src="colormap_black2green.png" alt="colormap_black2green.png"/>
<div class="caption">
Black to green, voltages</div></div>
  </td></tr>
<tr>
<td><div class="image">
<img src="colormap_default_spikes.png" alt="colormap_default_spikes.png"/>
<div class="caption">
Default, spikes</div></div>
 </td><td><div class="image">
<img src="colormap_black2green_spikes.png" alt="colormap_black2green_spikes.png"/>
<div class="caption">
Black to green, spikes</div></div>
  </td></tr>
</table>
<p>It is possible to modify the relative locations of the control points using <a href="python/api.html#rtneuron._rtneuron.ColorMap.setRange"><b>setRange</b></a>. This method sets the minimum and maximum values of the control points and adjusts the position of all the others preserving the relative distance between them (internally, this method is lightweight so it can be used to change the color map in interactively). The image below shows a simulation frame using the default color map and two range adjustments.</p>
<a class="anchor" id=""></a>
<table  class="collapsed_table">
<caption>Comparison of the same simulation frame using 3 different ranges adjustments of the default color map.</caption>
<tr>
<td><div class="image">
<img src="colormap_range_ex1.png" alt="colormap_range_ex1.png"/>
<div class="caption">
[-80, -10]</div></div>
 </td><td><div class="image">
<img src="colormap_range_ex2.png" alt="colormap_range_ex2.png"/>
<div class="caption">
[-70, -40]</div></div>
 </td><td><div class="image">
<img src="colormap_range_ex3.png" alt="colormap_range_ex3.png"/>
<div class="caption">
[-67, -48]</div></div>
  </td></tr>
</table>
<p>All images with voltage data come from time stamp 6 ms of the <code>allCompartments</code> report of the test data. For spike data the time stamp 10 ms was used.</p>
<p>For compartmental simulation data and targets whose coloring scheme is by width* the alpha channel of the final colors are computed by multiplying the alpha channel from the color map and the <em>branch width to color</em> function explained in <a class="el" href="basics.html#neuron_target_options">Neuron targets attributes</a>. The image below shows a comparison of the same simulation frame with and without <em>by width</em>:</p>
<a class="anchor" id=""></a>
<table  class="collapsed_table">
<caption>Comparison of the same simulation frame using the alpha channel from the color map and modulating it by the branch width.</caption>
<tr>
<td><div class="image">
<img src="colormap_default.png" alt="colormap_default.png"/>
<div class="caption">
Regular transparency</div></div>
 </td><td><div class="image">
<img src="colormap_alphabywidth.png" alt="colormap_alphabywidth.png"/>
<div class="caption">
Alpha modulated by width</div></div>
  </td></tr>
</table>
<h3><a class="anchor" id="per_object_colormaps"></a>
Per object color maps</h3>
<p>Color maps for simulation can also be specified on a neuron object handler basis using the Python API. This is true for both the top level scene handlers and subhandlers obtained with <a href="python/api.html#rtneuron._rtneuron.Scene.Object.query"><b>rtneuron.Scene.Object.query</b></a>. When a color map for compartmental data or spike activity is set at a handler, that color map overrides the color map from the view for the neurons contained by the handler. For temporary handlers the changes are persistent even after the handler is deleted.</p>
<p>To set a color map on a neuron object you have to first create an empty <a href="python/api.html#rtneuron._rtneuron.AttributeMap"><b>rtneuron.AttributeMap</b></a> on the object attribute <code><b>colormaps</b></code>. This attribute map is analagous to the <code><b>colormaps</b></code> attribute of Views. To assign a color map for compartments set the <a class="el" href="classbbp_1_1rtneuron_1_1ColorMap.html" title="Class to create color maps and store them as 1D texture of arbitrary size. ">ColorMap</a> object on the attribute <code><b>colormaps.compartments</b></code>, and for spikes assign it to <code><b>compartments.spikes</b></code> </p>
<p>Changes on the color maps are tracked by neuron object handlers, however they only become effective once the <a href="python/api.html#rtneuron._rtneuron.Scene.Object.update"><b>update</b></a> method of the handler is called. In the case there are neuron object handlers with overlapping neuron sets, the last one which was applied prevails.</p>
<p>Since it is not possible to remove attributes from an <a class="el" href="classbbp_1_1rtneuron_1_1AttributeMap.html" title="An attribute map that stores arbitrary values using strings as keys. ">AttributeMap</a>, the way to clear the color maps a subset of neurons is assigning (notice that there is no way of clearing a specific color map)</p>
<dl class="section warning"><dt>Warning</dt><dd>For implementation reasons, when a colormap is applied to the a neuron handler returned by <a href="python/api.html#rtneuron._rtneuron.Scene.Object.query"><b>query</b></a>, the handler must be kept alive to ensure correct rendering results. If the handler is destroyed, the results for spherical somas are undefined (the observable behaviour is that they will receive another colormap if the atlas is updated).</dd></dl>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;exc = ColorMap()</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;exc.setPoints({-80: [1, 0, 0, 0], -60: [1, 0, 0, 0.1], -10: [1, 1, 0, 1]})</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;view.attributes.colormaps.compartments = exc</div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;</div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;inh = ColorMap()</div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;inh.setPoints({-80: [0, 0, 1, 0], -60: [0, 0.2, 1, 0.1], -10: [0, 1, 1, 1]})</div><div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;all = view.scene.objects[0]</div><div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;subset = all.query(simulation.gids(<span class="stringliteral">&#39;Inhibitory&#39;</span>))</div><div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;subset.attributes.colormaps = AttributeMap()</div><div class="line"><a name="l00010"></a><span class="lineno">   10</span>&#160;subset.attributes.colormaps.compartments = inh</div><div class="line"><a name="l00011"></a><span class="lineno">   11</span>&#160;subset.update()</div></div><!-- fragment --><a class="anchor" id=""></a>
<table  class="collapsed_table">
<caption>Two renderings of the <em>Slice</em> target from /gpfs/bbp.cscs.ch/project/proj3/simulations/vizCa2p0_1x7/BlueConfig showing membrame voltage at somas.</caption>
<tr>
<td><div class="image">
<img src="default_colormap_somas.png" alt="default_colormap_somas.png"/>
<div class="caption">
Default colormaps</div></div>
 </td><td><div class="image">
<img src="multiple_colormap_somas.png" alt="multiple_colormap_somas.png"/>
<div class="caption">
Different colormaps for inhibitory and excitatory</div></div>
  </td></tr>
</table>
<h1><a class="anchor" id="cameras"></a>
Camera handling</h1>
<h2><a class="anchor" id="camera_object"></a>
The camera object</h2>
<p>Every <a href="python/api.html#rtneuron._rtneuron.View"><b>rtneuron.View</b></a> has an associated <a href="python/api.html#rtneuron._rtneuron.Camera"><b>rtneuron.Camera</b></a> which takes care of the projection type and parameters as well as the camera position and orientation.</p>
<p>The camera is created automatically for each view and cannot be replaced, but inside the Python shell there are methods to choose the projection type and camera position. Perspective projections can be specified using <a href="python/api.html#rtneuron._rtneuron.Camera.setProjectionFrustum"><b>setProjectionFrustum</b></a> and set with <a href="python/api.html#rtneuron._rtneuron.Camera.setProjectionPerspective"><b>setProjectionPerspective</b></a>. Orthographic projections are specified with <a href="python/api.html#rtneuron._rtneuron.Camera.setProjectionOrtho"><b>setProjectionOrtho</b></a>. The functions <a href="python/api.html#rtneuron._rtneuron.Camera.makeOrtho"><b>makeOrtho</b></a> and <a href="python/api.html#rtneuron._rtneuron.Camera.makePerspective"><b>makePerspective</b></a> can be used to switch between the two projection types,the parameters of each projection type and handled independently, so they are preserved when switching between one and the other. The camera position can be changed using <a href="python/api.html#rtneuron._rtneuron.Camera.setView"><b>setView</b></a> and <a href="python/api.html#rtneuron._rtneuron.Camera.setViewLookAt"><b>setViewLookAt</b></a>. The current view position and orientation can be recovered with <a href="python/api.html#rtneuron._rtneuron.Camera.getView"><b>getView</b></a>, this function returns a tuple <code>(position, (axis, angle))</code>, where the angle is in degrees. Follow the links to the reference for further details.</p>
<p>The default projection comes from the default Equalizer configuration. It is a perspective projection with a vertical field of view of 53º approximately.</p>
<dl class="section warning"><dt>Warning</dt><dd>Currently, view frustum culling and spherical somas do not work correctly with orthographic projections. It's advisable to use <code>&ndash;no-cuda</code> to work with orthographic projections.</dd></dl>
<h2><a class="anchor" id="camera_paths"></a>
Camera paths</h2>
<p>A camera path is a list of camera key frames (position, orientation and stereo correction factor) with a timestamp. Camera paths can be created and modified using the Python shell as well as loaded from and saved to files. When a camera is playing back a camera path, its position is updated frame by frame according to the playback parameters described below. This subsection describes how to create camera paths, how to play back camera paths from the command line as well as the Python shell is explained in <a class="el" href="basics.html#camera_path_manipulator">Camera path manipulator</a>.</p>
<h3><a class="anchor" id="camera_path_creation"></a>
Creating camera paths</h3>
<p>Camera paths can be created by hand, writing camera path files in a text editor, but this is only recommended for camera paths with 1 or 2 key frames. For camera paths consisting of several key frames the best option is to build them inside the Python shell and then saving the result to a file. Camera paths are represented, saved and loaded using the <a class="el" href="classbbp_1_1rtneuron_1_1CameraPath.html" title="A sequence of camera keyframes with timestamps. ">CameraPath</a> class.</p>
<p>A camera path is a collection of <a href="python/api.html#rtneuron._rtneuron.CameraPath.KeyFrame"><b>rtneuron.CameraPath.KeyFrame</b></a> objects. A key frame specifies a position, orientation and stereo correction. Once created, key frames can be added to a camera path using <a href="python/api.html#rtneuron._rtneuron.CameraPath.addKeyFrame"><b>addKeyFrame</b></a>, which takes a timestamp in <b>seconds</b> and a key frame. An alternative method is to call addKeyFrame with a view as input parameter. In this case all the parameters are taken from the view and its camera: </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;$ path = CameraPath()</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;$ k = CameraPath.KeyFrame()</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;$ k.position = [0, 0, 1000]</div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;$ <span class="comment"># k.orientation and k.stereoCorrection left at default values</span></div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;$ path.addKeyFrame(0, k)</div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;$ <span class="comment"># The non-default constructor takes exactly 3 arguments</span></div><div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;$ path.addKeyFrame(1, CameraPath.KeyFrame([0, 0, 2000], ([0, 0, 1], 0), 1))</div><div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;$ path.addKeyFrame(2, view)</div></div><!-- fragment --><p> Key frames can be queried, removed or replaced using their index in the path and the whole key frame list can be also extracted.</p>
<p>Camera paths can be saved and loaded using <a href="python/api.html#rtneuron._rtneuron.CameraPath.save"><b>rtneuron.CameraPath.save</b></a> and <a href="python/api.html#rtneuron._rtneuron.CameraPath.load"><b>rtneuron.CameraPath.load</b></a>.</p>
<h3><a class="anchor" id="camera_path_helpers"></a>
Predefined Camera paths</h3>
<p>Some predefined camera paths are already provided. These camera paths can be created using the functions found in the module rtneuron.util.camera.Paths**. The most relevant functions are:</p><ul>
<li><a href="python/api.html#rtneuron.util.camera.Paths.front_view"><b>rtneuron.util.camera.Paths.front_view(<em>blueconfig, targets, **kwargs</em>)</b></a> Creates a camera path with a top front view (y axis pointing up) of a target.</li>
<li><a href="python/api.html#rtneuron.util.camera.Paths.make_front_view"><b>rtneuron.util.camera.Paths.make_front_view(<em>view, **kwargs</em>)</b></a>: Computes a front view and applies it to the camera of a view.</li>
<li><a href="python/api.html#rtneuron.util.camera.Paths.top_view"><b>rtneuron.util.camera.Paths.top_view(<em>blueconfig, targets, **kwargs</em>)</b></a>: Creates a camera path looking from the top of a target down the negative y axis.</li>
<li><a href="python/api.html#rtneuron.util.camera.Paths.make_top_view"><b>rtneuron.util.camera.Paths.make_top_view(<em>view, **kwargs</em>)</b></a>: Computes a top view and applies it to the camera of a view.</li>
<li><a href="python/api.html#rtneuron.util.camera.Paths.flythrough"><b>rtneuron.util.camera.Paths.flythrough(<em>blueconfig, target, duration=10, **kwargs</em>)</b></a>: Creates a camera path with a camera travelling from a distant viewpoint to a close up near the centroid of the selected target. The travelling can be slowed down with optional parameters.</li>
<li><p class="startli"><a href="python/api.html#rtneuron.util.camera.Paths.front_to_top_rotation"><b>rtneuron.util.camera.Paths.front_to_top_rotation(<em>blue_config, targets, duration=10, **kwargs</em>)</b></a>: Creates a camera path with front view to top view rotation as shown below:</p>
<div class="image">
<img src="front_to_top_path.png" alt="front_to_top_path.png"/>
<div class="caption">
Front to top rotation example</div></div>
</li>
<li><p class="startli"><a href="python/api.html#rtneuron.util.camera.Paths.rotate_around"><b>rtneuron.util.camera.Paths.rotate_around(<em>blue_config, targets, duration=10, **kwargs</em>)</b></a>: Creates a camera path with a rotation around a target as shown below:</p>
<div class="image">
<img src="rotation_path.png" alt="rotation_path.png"/>
<div class="caption">
Rotate around example</div></div>
</li>
</ul>
<p>All the functions return a camera path, see the section on <a class="el" href="basics.html#camera_path_manipulator">camera path manipulators</a> to learn how to attach the camera path to a view. The functions <code>make_front_view</code> and <code>make_top_view</code> are different from the rest. These functions take a view as argument, and use it to inspect the scene attached to the view, create a key frame and apply it to the view (no camera path is created).</p>
<h3><a class="anchor" id="camera_path_recording"></a>
Recording camera paths</h3>
<p>It is possible to record a camera path while the camera is manipulated interactively. For that purpose there is a helper object called CameraPathRecorder. This object takes an engine and view in its constructor. To start recording a camera path call <code>startRecording</code>. Each time a frame is issued, the current camera position will be registered in a camera path. Use <code>stopRecording</code> to stop registering and <code>getCameraPath</code> to retrieve the path. Notice that key frames are only registered when a frame is issued, so once you stop moving the camera no spurious key frames will be generated before you can save it.</p>
<dl class="section note"><dt>Note</dt><dd>The programming reference for CameraPathRecorder is not available online, but it is present in the Python console.</dd></dl>
<h3><a class="anchor" id="camera_path_file_format"></a>
Camera path file format</h3>
<p>A camera path file is a plain text file with a timestamp in milliseconds and key frame per line with the following format:  <pre>
timestamp translation_x translation_y translation_z rotation_a rotation_b rotation_c rotation_d stereo_factor
</pre> All numbers are floating point numbers. The fields <code>rotation_(a,b,c,d)</code> are a quaternion that specifies the camera orientation as a rotation from the default orientation (looking down the z axis and being y the up vector). Let a be the rotation angle in radians and x, y, z the rotation axis, the rotation fields are: sin(a/2)*x sin(a/2)*y sin(a/2)*x cos(a/2). The <code>stereo_factor</code> field is a multiplicative factor which is applied to the inter-ocular distance (More about stereo is discussed <a class="el" href="advanced.html#stereo">this section</a>).</p>
<p>A camera path file looks like this:  <pre>
 0   4170 2329 1090 0.285096 0.568038 0.567425 0.523527 1
 250 4125 2449 1083 0.275059 0.573701 0.573202 0.516378 1
 500 4075 2567 1075 0.264962 0.579240 0.578855 0.509118 1
</pre></p>
<p>The python code to print a key frame in the format used by camera path files can be found below: </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;<span class="keyword">def </span>keyframe(view) :</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;  <span class="keyword">import</span> math</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;  pos, (axis, angle) = view.camera.getView()</div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;  half_angle = math.radians(angle * 0.5)</div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;  s = math.sin(half_angle)</div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;  <span class="keywordflow">return</span> (<span class="stringliteral">&quot;%f %f %f %f %f %f %f %f&quot;</span> % (</div><div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;              pos[0], pos[1], pos[2],</div><div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;              axis[0] * s, axis[1] * s, axis[2] * s, math.cos(half_angle),</div><div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;              view.attributes.stereo_correction))</div></div><!-- fragment --><h2><a class="anchor" id="camera_manipulators"></a>
Camera manipulators</h2>
<p>Camera manipulators are objects that handle frame, mouse and keyboard events <span id="note"><a><sup>note</sup><span><span>(</span>Or even
other devices in special cases<span>)</span></span></a></span> to modify the camera position and orientation. Each <a href="python/api.html#rtneuron._rtneuron.View"><b>rtneuron.View</b></a> contains a camera manipulator which is accessible by its property <b>cameraManipulator</b>. Manipulators can be replaced at any moment.</p>
<h3><a class="anchor" id="camera_tackball_manipulator"></a>
Trackball manipulator</h3>
<p>When a <a href="python/api.html#rtneuron._rtneuron.View"><b>rtneuron.View</b></a> is created a default camera manipulator is attached to it. This default manipulator emulates a trackball device with the mouse. The manipulator has an initial camera position and orientation and a reference point, these parameters are jointly known as the <code>home</code> <code>position</code>. Using the mouse it is possible to pan, travel and rotate the camera. For rotation move the mouse while pressing the left button, the camera will pivot on the reference point. For panning use the middle button, panning also moves the reference point for rotation. To move the camera back and forth use right button and move the mouse back and forth, the camera speed is proportional to distance to the reference point so, the closer to the point, the slower the camera moves. The camera and the reference point are reset to the home position when the space bar is pressed.</p>
<p>The keyboard can also be used to emulate the mouse. Use the cursor keys for camera panning. For rotations around the pivot point use they cursor keys with the Control key pressed. Use shift with the down and up cursors to move the camera back and forth.</p>
<p>If the scene attached to the view is updated, the home position is updated and the camera is reset to the initial position. The home position computed has the center of the circuit as the reference point, the y-axis as the up direction and places the camera along the z axis at a position where all somas fit in view. The auto update behaviour can be overridden setting the view attribute <code><b>auto_compute_home_position</b></code> to false. The home position can also be queried and changed manually in the Python shell with <a href="python/api.html#rtneuron._rtneuron.TrackballManipulator.getHomePosition"><b>getHomePosition</b></a> and <a href="python/api.html#rtneuron._rtneuron.TrackballManipulator.setHomePosition"><b>setHomePosition</b></a>.</p>
<h3><a class="anchor" id="camera_path_manipulator"></a>
Camera path manipulator</h3>
<p>There is a specific camera manipulator for camera paths, called <a href="python/api.html#rtneuron._rtneuron.CameraPathManipulator"><b>rtneuron.CameraPathManipulator</b></a>. This manipulator can be created and assigned to a view using the Python shell or initialized from the command line.</p>
<p>To load a camera path from the command line use the option <code>&ndash;path path_file</code>. This will create a camera manipulator that will be assigned to all views. Playback will start as soon as the scene shows up. By default, the camera path is played back using wall clock time, but if the option <code>&ndash;path-fps fps</code> is used, the camera path will advance 1/fps seconds each frame (regardless of the real rendering time).</p>
<p>When <code>&ndash;path</code> is used in combination with <code>&ndash;grab-frames</code>, the rendered frames will be recorded until the end of the camera path is reached (the rendering will continue looping, but no more frames will be recorded). Together with <code>&ndash;path-fps</code> and <code>&ndash;frame-count</code>, it is possible to record the exact frames needed to encode a movie of a camera path.</p>
<p>Within the Python shell the way to play back a camera path is to create a <a href="python/api.html#rtneuron._rtneuron.CameraPathManipulator"><b>rtneuron.CameraPathManipulator</b></a> object, assign the path to it with <a href="python/api.html#rtneuron._rtneuron.CameraPathManipulator.setPath"><b>setPath</b></a> and assign the manipulator to the <b>cameraManipulator</b> property of the target <a href="python/api.html#rtneuron._rtneuron.View"><b>rtneuron.View</b></a>. In a camera path manipulator it is possible to change the delay between camera path frames (wall clock or fixed), the playback start and stop time and the looping mode (swing, loop or no loop). Please see the <a href="python/api.html#rtneuron._rtneuron.CameraPathManipulator"><b>rtneuron.CameraPathManipulator</b></a> reference for further details.</p>
<p>Camera paths are played using spline interpolation for the camera position except for camera paths with only two key frames, in that case linear interpolation will be used (linear interpolation can be forced unconditionally setting the environmental variable RTNEURON_FORCE_LINEAR_PATH_INTERPOLATION to 1). For camera rotation, SLERP interpolation is always used in order to yield constant rotation velocity and minimal torque.</p>
<h3><a class="anchor" id="space_mouse_manipulator"></a>
SpaceMouse manipulator</h3>
<p>When RTNeuron is build with VRPN support it is possible to control the camera using a SpaceMouse device exported by a VRPN server. The folder where the server is set up will contain a <code>vrpn.cfg</code> file that holds the identifiers of the devices enabled by it. To configure the use of a SpaceMouse manipulator, this file must contain the line <code>vrpn_3DConnection_XXX device_id</code>, where <code>XXX</code> is the specific model of the SpaceMouse, e.g. <code>vrpn_3DConnection_Navigator_for_Notebooks device0</code>. Most devices are already present in the file and their configuration just needs to be uncommented. To start the server, execute <code>vrpn_server</code> and leave the process running.</p>
<p>The command line option <code>&ndash;use-spacemouse device_name@host</code> will enable the use of the SpaceMouse device as a camera manipulator, and specify the device name and host where it is located.</p>
<p>Inside the Python shell there is an object called <a href="python/api.html#rtneuron._rtneuron.VRPNManipulator"><b>rtneuron.VRPNManipulator</b></a>. This object takes in its constructor the device type and a device URL <code>device_name@host</code>. To use it, instantiate the object using <code>VRPNManipulator.DeviceType.SPACE_MOUSE</code> as device type and assign it to a view.</p>
<p>Similarly to the way the default camera manipulator (<a class="el" href="basics.html#camera_tackball_manipulator">Trackball manipulator</a>) works, the SpaceMouse manipulator has a home position. Pushing and pulling the device cap in any direction is translated into panning and zooming in or out of the 3D scene view; tilting and twisting can be used to orbit around the 3D model. The camera speed is proportional to distance to the reference point. The camera and the reference point can be reset by pressing any of the two physical buttons on the device sides.</p>
<h3><a class="anchor" id="wiimote_manipulator"></a>
Wiimote manipulator</h3>
<p>When RTNeuron is built with Wiimote support <span id="note"><a><sup>
note</sup><span><span>(</span> This is not the case of the official packages
actually works is difficult to describe and OSG dependent.
<span>)</span></span></a></span> it is possible to control the camera using a Wiimote device exported by a VRPN server. In this case, the line to be added or uncommented is <code>vrpn_WiiMote WiiMote0 1 0 0 1</code>. The command line option <code>&ndash;use-wiimote device_host@host</code> is used for this purpose.</p>
<p>As in the SpaceMouse case, <a href="python/api.html#rtneuron._rtneuron.VRPNManipulator"><b>rtneuron.VRPNManipulator</b></a> is used to create the camera manipulator for the Wiimote. Instantiate the object using <code>VRPNManipulator.DeviceType.WIIMOTE</code> as the first argument and the VRPN URL as the second one and assign it to a view.</p>
<p>In the case of the use of a Wiimote device, the camera can be manipulated by using exclusively the <code>Nunchuk</code> extension that can be attached to the main controller. By default, uniquely handling the main joystick will pan the camera, whereas using it in conjunction with the C button in the back of the controller will make it travel back and forth the z axis. On the other hand, if the joystick is handled while pressing the Z button below, the camera will rotate around the model.</p>
<h1><a class="anchor" id="movies"></a>
Frame capture and movie generation</h1>
<p>It is possible to save the frames rendered by RTNeuron into files using both command line options and the Python interpreter. For the highest quality results the interpreter mode is advised because idle mode anti-aliasing does not work in combination with frame grabbing for the command line options.</p>
<h2><a class="anchor" id="cli_frame_grabbing"></a>
Frame capture from the command line</h2>
<p>In order to capture frames from the command line, use the option <code>&ndash;grab-frames</code>. This option will make RTNeuron dump a file per view and rendered frame from the moment the circuit is loaded and displayed. By default, the files are dumped into the working directory using file names like <code>prefix_nnnnnn.png</code>, where prefix is <code>frame</code> if there a single view or <code>prefix_view_</code> if there are two or more views, being <code>view</code> the name of the view taken from the Equalizer configuration (the file format and common file prefix can be changed with <code>&ndash;file-format</code> and <code>&ndash;file-prefix</code>, the common file prefix can include an absolute path).</p>
<p>Frame recording will be finished in any of the following cases, whichever happens first (apart from engine exit):</p><ul>
<li>If a camera path with more than one key frame (i.e. the begin and end times are different) has been provided, when the camera path end is reached.</li>
<li>If a simulation report has been provided, when the end of the simulation window has been reached.</li>
<li>If the command line option <code>&ndash;frame-count <em>frames</em></code> has been given, after <em>frames</em> frames have been rendered starting from the first frame in which the circuit is displayed. In this case, the engine will also exit automatically. This option is ignored when <code>&ndash;shell</code> is also present in the command line.</li>
</ul>
<dl class="section note"><dt>Note</dt><dd>The results of the recording are undefined if the Equalizer layout is changed on-the-fly. The most probable outcome is that no frames will be captured for the views that were not active at engine start up. For more information on layouts (and other advanced Equalizer configuration) refer to <a class="el" href="advanced.html#display_config">Display configuration</a>.</dd></dl>
<h2><a class="anchor" id="console_frame_grabbing"></a>
Frame capture from the Python interpreter</h2>
<p>Inside the Python interpreter (or a Python script) there are 3 methods to dump the content of views into files. One of them is using <a href="python/api.html#rtneuron._rtneuron.RTNeuron.record"><b>rtneuron.RTNeuron.record</b></a> <a href="python/api.html#rtneuron._rtneuron.RecordingParams"><b>rtneuron.RecordingParams</b></a>, the other two require direct manipulation of view objects.</p>
<h3><a class="anchor" id="rtneuron_record"></a>
The RTNeuron.record function</h3>
<p>The <a href="python/api.html#rtneuron._rtneuron.RTNeuron.record"><b>rtneuron.RTNeuron.record</b></a> function takes an object with the recording parameters and triggers the generation and dumping of frames until one of the stop conditions is met. The recording parameters are given in a <a href="python/api.html#rtneuron._rtneuron.RecordingParams"><b>rtneuron.RecordingParams</b></a> object. The parameters configure:</p><ul>
<li>The camera path and whether frame recording must stop at the end of the camera path or not. The delta time between frames for the camera path can be set to be real-time or a fixed delay between frames.</li>
<li>The simulation time window and the delta time between simulation frames</li>
<li>The maximum number of frames to be rendered.</li>
<li>Additional parameters to set the file image prefix, format and output directory.</li>
</ul>
<p>Please refer to the <a href="python/api.html#rtneuron._rtneuron.RecordingParams">reference</a> to find the actual names, types and units of the parameters.</p>
<p>When <a href="python/api.html#rtneuron._rtneuron.RTNeuron.record"><b>rtneuron.RTNeuron.record</b></a> is called frame grabbing starts and frames are issued advancing the camera path and simulation time stamp automatically as required, until one of the stop criteria is met:</p><ul>
<li>The camera path end is reached and <code>stopAtCameraPathEnd</code> was set to true.</li>
<li>The end of the simulation window is reached.</li>
<li>The maximum number of frames to render is reached.</li>
</ul>
<p>Time intervals are open on the right (that means that from 0 to 1 s at 100 ms intervals, 10 frames will be rendered and not 11).</p>
<p>The function call is non-blocking, to wait for the last frame to be issued use <a href="python/api.html#rtneuron._rtneuron.RTNeuron.waitRecord"><b>rtneuron.RTNeuron.waitRecord</b></a>. This is the best way to guarantee that all frames are generated when running from a script instead of the interactive console.</p>
<p>This method does not considered the existence of several views. Despite frames will be dumped for all views, the file names used will collide, so the view from which frames are finally saved is undefined.</p>
<h3><a class="anchor" id="view_frame_grabbing_api"></a>
View API for frame grabbing</h3>
<p>A <a href="python/api.html#rtneuron._rtneuron.View"><b>rtneuron.View</b></a> object has two methods related to frame grabbing:</p>
<ul>
<li><a href="python/api.html#rtneuron._rtneuron.View.snapshot"><b>rtneuron.View.snapshot</b></a>: This methods takes an as input the name of the output file. It will trigger the rendering of a frame and write it to the output file. If idle anti-alias is enabled, the frame will be grabbed and dumped once all the accumulation steps are done (this behaviour can be changed through the <a class="el" href="classbbp_1_1rtneuron_1_1View.html" title="This class represents a view on a scene. ">View</a> attribute <code><b>snapshot_at_idle</b></code>), for this reason this is the recommended methods for production quality image generation. The file name used must include the extension, any file format supported by OpenSceneGraph will be accepted, for unsupported file formats an error message will be printed. In multichannel configurations, this method will only work on the first destination channel by default. If you want to capture a snapshot of all channels, add "%c" to the output file name. That string will be replaced by the channel name in order to generate one file per channel.</li>
<li><a href="python/api.html#rtneuron._rtneuron.View.record"><b>rtneuron.View.record</b></a>: This function can be used to turn on and off frame grabbing for a particular view. While frame grabbing is enabled, every rendered frame will be dumped to a file using the format <code>prefix_nnnnnn.png</code>, where <em>prefix</em> is the name of the view and <em>nnnnnn</em> is the frame number (if the name of the view in the Equalizer configuration is empty, "frame" will be used by default). The frame counter is reset to 0 every time record is enabled. The file name prefix and image file format of the image files generated can be configured modifying the view attributes <code><b>output_file_prefix</b></code> and <code><b>output_file_format</b></code>, when <code><b>output_file_prefix</b></code> is an empty string, the default prefix described before is used instead. </li>
</ul>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="index.html">Documentation</a></li><li class="navelem"><a class="el" href="user_guide.html">User Guide</a></li>
    <li class="footer">Generated on Wed Oct 10 2018 14:33:12 for RTNeuron by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.11 </li>
  </ul>
</div>
</body>
</html>
